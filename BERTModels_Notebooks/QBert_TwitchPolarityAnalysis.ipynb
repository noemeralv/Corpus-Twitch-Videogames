{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46644810",
   "metadata": {
    "id": "46644810"
   },
   "source": [
    "# Fine-tuning to a BERT-based model with classification layer: Polarity detection in videogames and Twitch\n",
    "\n",
    "The main idea is to use BERT models to tokenise texts to be classified by a new neural network, which will be placed at the output of the tokeniser. The idea of this is to \"specialise\" the classifier on the given task, in this case, classifying twitch comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z86ebKxdpOOV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 159920,
     "status": "ok",
     "timestamp": 1714986088411,
     "user": {
      "displayName": "juan pinto",
      "userId": "11813184739406666163"
     },
     "user_tz": -120
    },
    "id": "z86ebKxdpOOV",
    "outputId": "846ca66c-0a1a-4c34-c462-e6d66dc45a6e"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install torch torchvision\n",
    "!pip install scikit-learn\n",
    "!pip install matplotlib\n",
    "!pip install tqdm\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IdUrE_yDAwPF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5336,
     "status": "ok",
     "timestamp": 1714986257694,
     "user": {
      "displayName": "juan pinto",
      "userId": "11813184739406666163"
     },
     "user_tz": -120
    },
    "id": "IdUrE_yDAwPF",
    "outputId": "a1290c88-cc6d-468c-9ab5-9a7f092d2818"
   },
   "outputs": [],
   "source": [
    "#In case we use Google Colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5777fc28",
   "metadata": {
    "id": "5777fc28"
   },
   "source": [
    "## Preparing the database: Twitch and Video Games Corpus\n",
    "\n",
    "A classifier should have data associated with labels. As machines do not understand words directly, it is best to use numeric labels, such as natural numbers or one-hot encoding. The following code transforms the unique variables contained in the tag column and creates a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9280eba8",
   "metadata": {
    "id": "9280eba8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Upload CSV file\n",
    "df = pd.read_csv('CorpusTwitchVideogames2024FinalV2.csv')\n",
    "\n",
    "# Get the label columns\n",
    "labels = df['Polarity']\n",
    "\n",
    "# Get all unique labels and assign them a numeric value\n",
    "unique_labels = labels.unique()\n",
    "label_to_numeric = {label: i for i, label in enumerate(unique_labels)}\n",
    "\n",
    "# Create a new column with numeric values\n",
    "df['Numeric_Label'] = labels.map(label_to_numeric)\n",
    "\n",
    "#Label Positivo is Positive\n",
    "#Label Negativo is Negative\n",
    "#Label Indeterminado is Neutral\n",
    "\n",
    "# Function to convert numeric label back to text\n",
    "def numeric_to_text(numeric_label):\n",
    "    numeric_to_label = {v: k for k, v in label_to_numeric.items()}\n",
    "    return numeric_to_label[numeric_label]\n",
    "\n",
    "# Example of use of the function\n",
    "print(numeric_to_text(2))   # This will print the text corresponding to the numeric label 2\n",
    "\n",
    "# Save the modified DataFrame in a new CSV file\n",
    "df.to_csv('data_numeric_label_v3_polaridad.csv', index=False)\n",
    "\n",
    "print(\"A new column with numeric values has been created and the modified DataFrame has been saved in 'data_numeric_label.csv'.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0801ca31",
   "metadata": {
    "id": "0801ca31"
   },
   "source": [
    "# Fine-tuning BERT model with classification layer\n",
    "We load the created database. For this example we used current pytorch, it is suggested to use the version higher than 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8617dc",
   "metadata": {
    "id": "cd8617dc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Step 1: Load data from a CSV file\n",
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    texts = data['Text'].tolist()\n",
    "    labels = data['Numeric_Label'].tolist()\n",
    "    return texts, labels\n",
    "\n",
    "# Path to CSV file\n",
    "file_path = 'data_numeric_label_v3_polaridad.csv'  # Replace with the path to your CSV file\n",
    "\n",
    "# Load data\n",
    "texts, labels = load_data(file_path)\n",
    "\n",
    "# Step 2: Initialise BERT tokeniser and load pre-trained model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Step 3: Tokenise and encrypt the data\n",
    "encodings = tokenizer(texts, truncation=True, padding=True)\n",
    "\n",
    "# Step 4: Create PyTorch dataset\n",
    "dataset = TensorDataset(torch.tensor(encodings['input_ids']),\n",
    "                        torch.tensor(encodings['attention_mask']),\n",
    "                        torch.tensor(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ecc7ad",
   "metadata": {
    "id": "78ecc7ad"
   },
   "source": [
    "## Model Training\n",
    "A model based on \"Bertsequenceclassificator\" is trained to ensure that the pipeline of tokenised data is faster to implement. This model can be fine-tuned using classical methods (hyperparameter settings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6547b9",
   "metadata": {
    "id": "8b6547b9"
   },
   "outputs": [],
   "source": [
    "# Step 5: Configure k-fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)  # We change the K-fold value according to our model\n",
    "\n",
    "# Lists for storing metrics\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "confusion_matrices = []\n",
    "\n",
    "# Step 6: Train and evaluate the model using k-fold cross-validation\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=max(labels)+1).to(device)  \n",
    "# Number of labels is the maximum numeric value in the Numeric_Label column.\n",
    "optimizer = AdamW(model.parameters(), lr=1e-6)\n",
    "\n",
    "for fold, (train_indices, test_indices) in enumerate(kf.split(dataset)):\n",
    "    train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "    test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(8):  # number of epoch to train the model\n",
    "        for batch in tqdm(train_loader, desc=\"Fold {} - Epoch {}\".format(fold+1, epoch + 1)):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predicted_labels = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Fold {} - Evaluation\".format(fold+1)):\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            predicted_labels.extend(predicted.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = correct / total\n",
    "    accuracies.append(accuracy)\n",
    "    precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
    "    precisions.append(precision)\n",
    "    recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "    recalls.append(recall)\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    print(\"Metrics in the test set (Fold {}):\".format(fold+1))\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1-score:\", f1)\n",
    "\n",
    "    # Calculating and storing the confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    confusion_matrices.append(cm)\n",
    "\n",
    "    # Save the training model to be used in the future\n",
    "    torch.save(model.state_dict(), 'bert_model_V9_Polaridad_fold_{}.pth'.format(fold+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461cbda7",
   "metadata": {
    "id": "461cbda7"
   },
   "source": [
    "## Evaluation of the trained BERT model with the classification layer\n",
    "The evaluation is made with the data set not considered during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82f0a8c",
   "metadata": {
    "id": "a82f0a8c"
   },
   "outputs": [],
   "source": [
    "# Calculate and display the average confusion matrix as a heat map with Matplotlib\n",
    "average_cm = np.mean(confusion_matrices, axis=0)\n",
    "average_cm = np.round(average_cm).astype(int)\n",
    "\n",
    "# Define label names\n",
    "labels_names = [\"Positivo\", \"Negativo\", \"Indeterminado\"]\n",
    "\n",
    "#Label Positivo is Positive\n",
    "#Label Negativo is Negative\n",
    "#Label Indeterminado is Neutral\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(average_cm, interpolation='nearest', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(labels_names))\n",
    "plt.xticks(tick_marks, labels_names, rotation=45)\n",
    "plt.yticks(tick_marks, labels_names)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('Real')\n",
    "plt.xlabel('Predicted')\n",
    "\n",
    "# Show the values of the confusion matrix in each cell\n",
    "width, height = average_cm.shape\n",
    "for x in range(width):\n",
    "    for y in range(height):\n",
    "        plt.annotate(str(average_cm[x][y]), xy=(y, x), horizontalalignment='center', verticalalignment='center')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Show average metrics\n",
    "print(\"\\nAverage metrics across all folds:\")\n",
    "print(\"Average accuracy:\", sum(accuracies) / len(accuracies))\n",
    "print(\"Average Precision:\", sum(precisions) / len(precisions))\n",
    "print(\"Average Recall:\", sum(recalls) / len(recalls))\n",
    "print(\"Average F1-score:\", sum(f1_scores) / len(f1_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ba3d4b",
   "metadata": {
    "id": "d8ba3d4b"
   },
   "source": [
    "## Emotional response prediction of the trained BERT model\n",
    "Give a few sentences and test how you classify the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73621154",
   "metadata": {
    "id": "73621154"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Load the pre-trained model\n",
    "model_path = 'bert_model_V9_Polaridad_fold_6.pth' \n",
    "# Ensure that the number of labels is the same as the number used during training\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3) \n",
    "\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "#If no GPU is used add map_location=torch.device('cpu')\n",
    "model.eval()\n",
    "\n",
    "# Initialising the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Text classification function\n",
    "def classify_text(text):\n",
    "    # Tokenise and encode text\n",
    "    encoded_text = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    # Passing text through the model\n",
    "    outputs = model(**encoded_text)\n",
    "\n",
    "    # Getting the predictions\n",
    "    _, predicted_class = torch.max(outputs.logits, 1)\n",
    "\n",
    "    return predicted_class.item()\n",
    "\n",
    "# Example to predict\n",
    "\n",
    "#Label Positivo is Positive\n",
    "#Label Negativo is Negative\n",
    "#Label Indeterminado is Neutral\n",
    "while True:\n",
    "    user_input = input(\"Enter a phrase to classify (or 'exit' to exit): \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "    else:\n",
    "        label_id = classify_text(user_input)\n",
    "        labels_names = [\"Positivo\", \"Negativo\", \"Indeterminado\"]\n",
    "        print(\"The sentece '{}' is classified as: {}\".format(user_input, labels_names[label_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ZWumiDUpkNn",
   "metadata": {
    "id": "3ZWumiDUpkNn"
   },
   "source": [
    "## Complete training code in one step for 3 polarities: Positive, Negative, Neutral\n",
    "All the above steps in one unified notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OfbMRKL1pRih",
   "metadata": {
    "id": "OfbMRKL1pRih"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Step 1: Load data from a CSV file\n",
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    texts = data['Text'].tolist()\n",
    "    labels = data['Numeric_Label'].tolist()\n",
    "    return texts, labels\n",
    "\n",
    "# Path to CSV file\n",
    "file_path = 'data_numeric_label_v3_polaridad.csv'  # Replace with the path to your CSV file\n",
    "\n",
    "# Load Data\n",
    "texts, labels = load_data(file_path)\n",
    "\n",
    "# Step 2: Initialise BERT tokeniser and load pre-trained model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Step 3: Tokenise and encrypt the data\n",
    "encodings = tokenizer(texts, truncation=True, padding=True)\n",
    "\n",
    "# Step 4: Create PyTorch dataset\n",
    "dataset = TensorDataset(torch.tensor(encodings['input_ids']),\n",
    "                        torch.tensor(encodings['attention_mask']),\n",
    "                        torch.tensor(labels))\n",
    "\n",
    "# Step 5: Configure k-fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)  \n",
    "\n",
    "# Lists for storing metrics\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "confusion_matrices = []\n",
    "\n",
    "# Step 6: Train and evaluate the model using k-fold cross-validation\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=max(labels)+1).to(device)  \n",
    "# Number of labels is the maximum numeric value in the Numeric_Label column\n",
    "optimizer = AdamW(model.parameters(), lr=1e-6)\n",
    "\n",
    "for fold, (train_indices, test_indices) in enumerate(kf.split(dataset)):\n",
    "    train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "    test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(8):  # # 8 training epoch in this case\n",
    "        for batch in tqdm(train_loader, desc=\"Fold {} - Época {}\".format(fold+1, epoch + 1)):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predicted_labels = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Fold {} - Evaluation\".format(fold+1)):\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            predicted_labels.extend(predicted.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = correct / total\n",
    "    accuracies.append(accuracy)\n",
    "    precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
    "    precisions.append(precision)\n",
    "    recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "    recalls.append(recall)\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    print(\"Metrics in the test set(Fold {}):\".format(fold+1))\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1-score:\", f1)\n",
    "\n",
    "    # Calculating and storing the confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    confusion_matrices.append(cm)\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), 'bert_model_V9_Polaridad_fold_{}.pth'.format(fold+1))\n",
    "\n",
    "# Calculate and display the average confusion matrix as a heat map with Matplotlib\n",
    "average_cm = np.mean(confusion_matrices, axis=0)\n",
    "average_cm = np.round(average_cm).astype(int)\n",
    "\n",
    "# Define the labels\n",
    "labels_names = [\"Positivo\", \"Negativo\", \"Indeterminado\"]\n",
    "\n",
    "\n",
    "#Label Positivo is Positive\n",
    "#Label Negativo is Negative\n",
    "#Label Indeterminado is Neutral\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(average_cm, interpolation='nearest', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(labels_names))\n",
    "plt.xticks(tick_marks, labels_names, rotation=45)\n",
    "plt.yticks(tick_marks, labels_names)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('Real')\n",
    "plt.xlabel('Predicted')\n",
    "\n",
    "# Display confusion matrix values in each cell\n",
    "width, height = average_cm.shape\n",
    "for x in range(width):\n",
    "    for y in range(height):\n",
    "        plt.annotate(str(average_cm[x][y]), xy=(y, x), horizontalalignment='center', verticalalignment='center')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Display average metrics\n",
    "print(\"\\nAverage metrics across all folds:\")\n",
    "print(\"Average accuracy:\", sum(accuracies) / len(accuracies))\n",
    "print(\"Average Precision:\", sum(precisions) / len(precisions))\n",
    "print(\"Average Recall:\", sum(recalls) / len(recalls))\n",
    "print(\"Average F1-score:\", sum(f1_scores) / len(f1_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80-XqYZOZjS9",
   "metadata": {
    "id": "80-XqYZOZjS9"
   },
   "source": [
    "# Obtaining trained model metrics, both globally and by class: 3 polarities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k1rBtESuZinx",
   "metadata": {
    "id": "k1rBtESuZinx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define the device to be used (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load tokeniser\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Load the pre-trained model\n",
    "model_path = 'bert_model_V9_Polaridad_fold_6.pth'\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Loading the database from a CSV file\n",
    "data = pd.read_csv(\"data_numeric_label_v3_polaridad.csv\")  # Replace with the actual path of your CSV file\n",
    "\n",
    "# Split data into features (X) and labels (y)\n",
    "X = data['Text']  # SAssuming you have a column called \"Text\" that contains your data\n",
    "y = data['Numeric_Label']  # Assuming you have a column called \"Numeric_Label\" that contains the labels\n",
    "\n",
    "# Prepare inputs and labels as tensioners\n",
    "inputs = tokenizer(list(X), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "labels = torch.tensor(y.values)\n",
    "\n",
    "# Create a DataLoader to iterate over data with a progress bar\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], labels)\n",
    "dataloader = DataLoader(dataset, batch_size=8)\n",
    "\n",
    "# Initialising metrics\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "# Making inference with the progress bar\n",
    "for batch in tqdm(dataloader, desc=\"Inferencia\"):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    inputs = {'input_ids': batch[0], 'attention_mask': batch[1]}\n",
    "    labels = batch[2]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    true_labels.extend(labels.cpu().numpy())\n",
    "    predicted_labels.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "\n",
    "# Calculatinog metrics\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "report = classification_report(true_labels, predicted_labels, target_names=[\"Positivo\", \"Negativo\", \"Indeterminado\"])\n",
    "\n",
    "# Calculate the confusion matrix by class\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Calculate accuracy, recall and F1-score for each class\n",
    "class_metrics = classification_report(true_labels, predicted_labels, target_names=[\"Positivo\", \"Negativo\", \"Indeterminado\"], output_dict=True)\n",
    "\n",
    "# Print results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification report:\")\n",
    "print(report)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Class metrics:\")\n",
    "for clase, metrics in class_metrics.items():\n",
    "    if clase != 'Accuracy':  # Exclude the global accuracy metric\n",
    "        print(f\"Clase {clase}:\")\n",
    "        print(f\"  Precisión: {metrics['precision']}\")\n",
    "        print(f\"  Recall: {metrics['recall']}\")\n",
    "        print(f\"  F1-score: {metrics['f1-score']}\")\n",
    "\n",
    "# Visualising the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Positivo\", \"Negativo\", \"Indeterminado\"], yticklabels=[\"Positivo\", \"Negativo\", \"Indeterminado\"])\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"Real labels\")\n",
    "plt.title(\"Visulisation of the Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "#Label Positivo is Positive\n",
    "#Label Negativo is Negative\n",
    "#Label Indeterminado is Neutral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zdZ2aceuDYvj",
   "metadata": {
    "id": "zdZ2aceuDYvj"
   },
   "source": [
    "## Complete training code for 2 polarities: Positive, Negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CXKXRn_hDYcX",
   "metadata": {
    "id": "CXKXRn_hDYcX"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Step 1: Load data from a CSV file\n",
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    texts = data['Text'].tolist()\n",
    "    labels = data['Numeric_Label_polarity'].tolist()\n",
    "    return texts, labels\n",
    "\n",
    "# Path to CSV file\n",
    "file_path = 'data_numeric_label_v5_polaridad.csv'  # Replace with the path to your CSV file\n",
    "\n",
    "# Load Data\n",
    "texts, labels = load_data(file_path)\n",
    "\n",
    "# Step 2: Initialise BERT tokeniser and load pre-trained model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Step 3: Tokenise and encrypt the data\n",
    "encodings = tokenizer(texts, truncation=True, padding=True)\n",
    "\n",
    "# Step 4: Create PyTorch dataset\n",
    "dataset = TensorDataset(torch.tensor(encodings['input_ids']),\n",
    "                        torch.tensor(encodings['attention_mask']),\n",
    "                        torch.tensor(labels))\n",
    "\n",
    "# Step 5: Configure k-fold cross-validation\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)  # We can change the value of K-fold\n",
    "\n",
    "# Lists of metrics\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "confusion_matrices = []\n",
    "\n",
    "# Step 6: Train and evaluate the model using k-fold cross-validation\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=max(labels)+1).to(device)  # Número de etiquetas es el máximo valor numérico en la columna Numeric_Label\n",
    "optimizer = AdamW(model.parameters(), lr=1e-6)\n",
    "\n",
    "for fold, (train_indices, test_indices) in enumerate(kf.split(dataset)):\n",
    "    train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "    test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(3):  # 8 epoch in this case\n",
    "        for batch in tqdm(train_loader, desc=\"Fold {} - Epoch {}\".format(fold+1, epoch + 1)):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predicted_labels = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Fold {} - Evaluation\".format(fold+1)):\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            predicted_labels.extend(predicted.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = correct / total\n",
    "    accuracies.append(accuracy)\n",
    "    precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
    "    precisions.append(precision)\n",
    "    recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "    recalls.append(recall)\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    print(\"Metrics in the test set(Fold {}):\".format(fold+1))\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1-score:\", f1)\n",
    "\n",
    "    # Calculating and storing the confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    confusion_matrices.append(cm)\n",
    "\n",
    "    # Save the training model\n",
    "    torch.save(model.state_dict(), 'bert_model_V11_Polaridad_fold_{}.pth'.format(fold+1))\n",
    "\n",
    "# Calculate and display the average confusion matrix as a heat map with Matplotlib\n",
    "average_cm = np.mean(confusion_matrices, axis=0)\n",
    "average_cm = np.round(average_cm).astype(int)\n",
    "\n",
    "# Define label names\n",
    "labels_names = [\"Negativo\",\"Positivo\"]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(average_cm, interpolation='nearest', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(labels_names))\n",
    "plt.xticks(tick_marks, labels_names, rotation=45)\n",
    "plt.yticks(tick_marks, labels_names)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('Real')\n",
    "plt.xlabel('Predicted')\n",
    "\n",
    "# Display confusion matrix values in each cell\n",
    "width, height = average_cm.shape\n",
    "for x in range(width):\n",
    "    for y in range(height):\n",
    "        plt.annotate(str(average_cm[x][y]), xy=(y, x), horizontalalignment='center', verticalalignment='center')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Display average metrics\n",
    "print(\"\\nAverage metrics across all folds:\")\n",
    "print(\"Average accuracy:\", sum(accuracies) / len(accuracies))\n",
    "print(\"Average Precision:\", sum(precisions) / len(precisions))\n",
    "print(\"Average Recall:\", sum(recalls) / len(recalls))\n",
    "print(\"Average F1-score:\", sum(f1_scores) / len(f1_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9LrJtvsgDkfS",
   "metadata": {
    "id": "9LrJtvsgDkfS"
   },
   "source": [
    "## Emotional response prediction of the trained BERT model with polarity\n",
    "Give a few sentences and test how you classify the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Cr39YZXEDj4b",
   "metadata": {
    "id": "Cr39YZXEDj4b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Define the device to be used (GPU if available, otherwise CPU).\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the pre-trained model\n",
    "model_path = 'bert_model_V10_Polaridad_fold_7.pth' \n",
    "\n",
    "# Make sure that the number of labels is the same as the number used during the training\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)  \n",
    "\n",
    "#model.load_state_dict(torch.load(model_path))\n",
    "#If no GPU is used add map_location=torch.device('cpu')\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "# Initialising the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Text classification function\n",
    "def classify_text(text):\n",
    "    # Tokenise and encode text\n",
    "    encoded_text = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    # Passing text through the model\n",
    "    outputs = model(**encoded_text)\n",
    "\n",
    "    # Getting the predictions\n",
    "    _, predicted_class = torch.max(outputs.logits, 1)\n",
    "\n",
    "    return predicted_class.item()\n",
    "\n",
    "# Example to predict the polarity of a sentence\n",
    "while True:\n",
    "    user_input = input(\"Enter a sentence to classify (or 'exit' to exit): \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "    else:\n",
    "        label_id = classify_text(user_input)\n",
    "        labels_names = [\"Negativo\", \"Positivo\"]\n",
    "        print(\"The sentence '{}' Has been classified as: {}\".format(user_input, labels_names[label_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c-pXHEjDp08",
   "metadata": {
    "id": "6c-pXHEjDp08"
   },
   "source": [
    "# Model metrics, both globally and by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb38305-1f94-43b7-8a6c-d9121bde2fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define the device to be used (GPU if available, otherwise CPU).\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Load tokeniser\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Load the pre-trained model\n",
    "model_path = '/content/drive/MyDrive/BERTAttentMask_SM/bert_model_V9_Polaridad_fold_6.pth'\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Loading the database from a CSV file\n",
    "# Replace with the actual path of your CSV file\n",
    "\n",
    "\n",
    "# Split data into features (X) and labels (y)\n",
    "X = data['Text']  # Assuming you have a column called \"Text\" that contains your data\n",
    "y = data['Numeric_Label']  # Assuming you have a column called \"Numeric_Label\" that contains the labels\n",
    "\n",
    "# Split data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# prepare inputs and labels as tensioners\n",
    "train_inputs = tokenizer(list(X_train), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "train_labels = torch.tensor(y_train.values)\n",
    "test_inputs = tokenizer(list(X_test), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "test_labels = torch.tensor(y_test.values)\n",
    "\n",
    "# Create DataLoaders to iterate over data with a progress bar\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)\n",
    "test_dataset = TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'], test_labels)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "# Initialising metrics\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "# Performing inference with the test set\n",
    "for batch in tqdm(test_dataloader, desc=\"Inference on test set\"):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    inputs = {'input_ids': batch[0], 'attention_mask': batch[1]}\n",
    "    labels = batch[2]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    true_labels.extend(labels.cpu().numpy())\n",
    "    predicted_labels.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "report = classification_report(true_labels, predicted_labels, target_names=[\"Positivo\", \"Negativo\", \"Indeterminado\"])\n",
    "\n",
    "# Calculate the confusion matrix by class\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Calculate accuracy, recall and F1-score for each class\n",
    "class_metrics = classification_report(true_labels, predicted_labels, target_names=[\"Positivo\", \"Negativo\", \"Indeterminado\"], output_dict=True)\n",
    "\n",
    "# Print results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification report:\")\n",
    "print(report)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Class metrics:\")\n",
    "for clase, metrics in class_metrics.items():\n",
    "    if clase != 'accuracy':  # Exclude the global accuracy metric\n",
    "        print(f\"Class {clase}:\")\n",
    "        print(f\"  Precision: {metrics['precision']}\")\n",
    "        print(f\"  Recall: {metrics['recall']}\")\n",
    "        print(f\"  F1-score: {metrics['f1-score']}\")\n",
    "\n",
    "# Visualising the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='YlGnBu', xticklabels=[\"Positivo\", \"Negativo\", \"Indeterminado\"], yticklabels=[\"Positivo\", \"Negativo\", \"Indeterminado\"])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
